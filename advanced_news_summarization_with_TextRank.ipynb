{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vasil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vasil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from itertools import product, count\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Similarity ###\n",
    "Calculate the similarity between two sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank Algorithm\n",
    "\n",
    "$$Similarity\\left( S _ { i } , S _ { j } \\right) = \\frac { \\left| \\left\\{ w _ { k } | w _ { k } \\in S _ { i } \\& w _ { k } \\in S _ { j } \\right\\} \\right| } { \\log \\left( \\left| S _ { i } \\right| \\right) + \\log \\left( \\left| S _ { j } \\right| \\right) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S _ { i }$ represents the ith sentence <br>\n",
    "$w _ { k }$ represents the kth word in a sentence <br>\n",
    "$|S _ { i }|$ represents the number of words in a sentence <br>\n",
    "${ \\left\\{ w _ { k } | w _ { k } \\in S _ { i } \\& w _ { k } \\in S _ { j } \\right\\} }$ represents the number of words appear in both $S _ { i }$ and $S _ { j }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(sen1, sen2):\n",
    "    counter = 0\n",
    "    for word in sen1:\n",
    "        if word in sen2:\n",
    "            counter += 1\n",
    "    return counter / (math.log(len(sen1)) + math.log(len(sen2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Similarity Adjacency Matrix ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "传入句子的列表\n",
    "返回各个句子之间相似度的图\n",
    "（邻接矩阵表示）\n",
    "\"\"\"\n",
    "def create_graph(word_sent):\n",
    "    num = len(word_sent)\n",
    "    # 初始化表\n",
    "    board = [[0.0 for _ in range(num)] for _ in range(num)]\n",
    "\n",
    "    for i, j in product(range(num), repeat=2):\n",
    "        if i != j:\n",
    "            board[i][j] = calculate_similarity(word_sent[i], word_sent[j])\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the similarity algorithm, we can calculate the frequency score with TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W S \\left( V _ { i } \\right) = ( 1 - d ) + d * \\sum _ { V _ { j } \\in \\operatorname { In } \\left( V _ { i } \\right) } \\frac { w _ { j i } } { \\sum _ { V _ { k } \\in O u t \\left( V _ { j } \\right) } w _ { j k } } W S \\left( V _ { j } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W S \\left( V _ { i } \\right)$ represents the score of sentence $V _ { i }$ <br>\n",
    "$d$ is the Damping Constant,  $(0<d<1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "输入相似度邻接矩阵\n",
    "返回各个句子的分数\n",
    "\"\"\"\n",
    "def weighted_pagerank(weight_graph):\n",
    "    # 把初始的分数值设置为0.5\n",
    "    scores = [0.5 for _ in range(len(weight_graph))]\n",
    "    old_scores = [0.0 for _ in range(len(weight_graph))]\n",
    "\n",
    "    # 开始迭代\n",
    "    while different(scores, old_scores):\n",
    "        for i in range(len(weight_graph)):\n",
    "            old_scores[i] = scores[i]\n",
    "\n",
    "        for i in range(len(weight_graph)):\n",
    "            scores[i] = calculate_score(weight_graph, scores, i)\n",
    "    return scores\n",
    "\n",
    "\"\"\"\n",
    "判断前后分数有没有变化\n",
    "前后差距小于0.0001\n",
    "分数就趋于稳定\n",
    "\"\"\"\n",
    "def different(scores, old_scores):\n",
    "    flag = False\n",
    "    for i in range(len(scores)):\n",
    "        if math.fabs(scores[i] - old_scores[i]) >= 0.0001:\n",
    "            flag = True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\"\"\"\n",
    "根据公式求出指定句子的分数\n",
    "\"\"\"\n",
    "def calculate_score(weight_graph, scores, i):\n",
    "    length = len(weight_graph)\n",
    "    d = 0.85\n",
    "    added_score = 0.0\n",
    "\n",
    "    for j in range(length):\n",
    "        fraction = 0.0\n",
    "        denominator = 0.0\n",
    "        # 计算分子\n",
    "        fraction = weight_graph[j][i] * scores[j]\n",
    "        # 计算分母\n",
    "        for k in range(length):\n",
    "            denominator += weight_graph[j][k]\n",
    "        added_score += fraction / denominator\n",
    "    # final score\n",
    "    weighted_score = (1 - d) + d * added_score\n",
    "\n",
    "    return weighted_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find n sentences with highest scores ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summarize(text,n):\n",
    "    # tokenize sentences\n",
    "    sents = sent_tokenize(text)\n",
    "    # tokenize words\n",
    "    # word_sent is a 2D list\n",
    "    # word_sent[i] represents the ith sentence\n",
    "    # word_sent[i][j] represents the jth word in the ith sentence\n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "\n",
    "    # filter out stopwords\n",
    "    for i in range(len(word_sent)):\n",
    "        for word in word_sent[i]:\n",
    "            if word in stopwords:\n",
    "                word_sent[i].remove(word)\n",
    "    similarity_graph = create_graph(word_sent)\n",
    "    scores = weighted_pagerank(similarity_graph)\n",
    "    sent_selected = nlargest(n, zip(scores, count()))\n",
    "    sent_index = []\n",
    "    for i in range(n):\n",
    "        sent_index.append(sent_selected[i][1])\n",
    "    return [sents[i] for i in sent_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The PHE website and app has a quiz that gives users a health score based on their lifestyle habits by asking questions such as, \"Which snacks do you eat in a normal day?\"', 'The campaign\\'s clinical adviser, Prof Muir Gray, said it was about trying to make people have a different attitude to an \"environmental problem\".']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open(\"news.txt\", \"r\") as myfile:\n",
    "        text = myfile.read().replace('\\n' , '')\n",
    "    print(Summarize(text, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
